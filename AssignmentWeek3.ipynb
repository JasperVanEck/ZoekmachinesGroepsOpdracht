{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoekmachines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by\n",
    "\n",
    "__Name__: Ghislaine van den Boogerd\n",
    "\n",
    "__Student id__ : 10996087"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toelichting\n",
    "\n",
    "* De meeste opgaven worden automatisch nagekeken. Bij vrijwel alle opdrachten staan er een paar zichtbare tests onder de opdracht, dit is voornamelijk om te zorgen dat je de juiste type output geeft. De andere tests worden na inleveren toegevoegd aan die cell.\n",
    "\n",
    "## Voor het inleveren!\n",
    "\n",
    "* Pas niet de cellen aan, vooral niet die je niet kunt editen. Dit levert problemen op bij nakijken. Twijfel je of je per ongeluk iets hebt gewijzigd, kopieer dan bij inleveren je antwoorden naar een nieuw bestand, zodat het niet fout kan gaan.\n",
    "\n",
    "* Zorg dat de code goed runt van boven naar beneden, verifieer dat door boven in Kernel -> Restart & Run All uit te voeren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Week-3\" data-toc-modified-id=\"Week-3-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Week 3</a></span></li><li><span><a href=\"#Questions-from-Chapter-8-in-MRS\" data-toc-modified-id=\"Questions-from-Chapter-8-in-MRS-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Questions from Chapter 8 in MRS</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#8.1\" data-toc-modified-id=\"8.1-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>8.1</a></span></li><li><span><a href=\"#8.2\" data-toc-modified-id=\"8.2-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>8.2</a></span></li><li><span><a href=\"#8.3\" data-toc-modified-id=\"8.3-2.0.3\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>8.3</a></span></li><li><span><a href=\"#8.4\" data-toc-modified-id=\"8.4-2.0.4\"><span class=\"toc-item-num\">2.0.4&nbsp;&nbsp;</span>8.4</a></span></li><li><span><a href=\"#8.8\" data-toc-modified-id=\"8.8-2.0.5\"><span class=\"toc-item-num\">2.0.5&nbsp;&nbsp;</span>8.8</a></span></li><li><span><a href=\"#8.8-extra\" data-toc-modified-id=\"8.8-extra-2.0.6\"><span class=\"toc-item-num\">2.0.6&nbsp;&nbsp;</span>8.8 extra</a></span></li><li><span><a href=\"#8.9\" data-toc-modified-id=\"8.9-2.0.7\"><span class=\"toc-item-num\">2.0.7&nbsp;&nbsp;</span>8.9</a></span></li><li><span><a href=\"#8.10\" data-toc-modified-id=\"8.10-2.0.8\"><span class=\"toc-item-num\">2.0.8&nbsp;&nbsp;</span>8.10</a></span></li></ul></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Evaluation</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Evaluation-measures\" data-toc-modified-id=\"Evaluation-measures-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Evaluation measures</a></span></li><li><span><a href=\"#Precision-recall-curve\" data-toc-modified-id=\"Precision-recall-curve-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Precision recall curve</a></span></li><li><span><a href=\"#Cohen's-Kappa\" data-toc-modified-id=\"Cohen's-Kappa-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>Cohen's Kappa</a></span></li><li><span><a href=\"#10.1-Creating-a-test-set\" data-toc-modified-id=\"10.1-Creating-a-test-set-3.0.4\"><span class=\"toc-item-num\">3.0.4&nbsp;&nbsp;</span>10.1 Creating a test set</a></span></li><li><span><a href=\"#10.2-Assess-search-engine-results\" data-toc-modified-id=\"10.2-Assess-search-engine-results-3.0.5\"><span class=\"toc-item-num\">3.0.5&nbsp;&nbsp;</span>10.2 Assess search engine results</a></span></li></ul></li></ul></li><li><span><a href=\"#Programming-(Inverted-Index)\" data-toc-modified-id=\"Programming-(Inverted-Index)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Programming (Inverted Index)</a></span><ul class=\"toc-item\"><li><span><a href=\"#12-Positional-inverted-index\" data-toc-modified-id=\"12-Positional-inverted-index-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>12 Positional inverted index</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "36d7e1f5bd8f35d837a0d92a79995ce9",
     "grade": false,
     "grade_id": "cell-0e1219abb9e62568",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "af12c8051c04438a128342ea6f2cee92",
     "grade": false,
     "grade_id": "cell-738a659fa61ddb20",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal, assert_almost_equal\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "128135c854ecdb8bf8f0f29376bc4ab5",
     "grade": false,
     "grade_id": "cell-65656d2217b99b63",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Questions from Chapter 8 in MRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a1960a516c228db42ab3e6df06fa6f44",
     "grade": false,
     "grade_id": "cell-a65f837aecb68efb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 8.1\n",
    "An IR system returns 8 relevant documents, and 10 nonrelevant documents. There are a total of 20 relevant documents in the collection. What is the precision of the system on this search, and what is its recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b66b1d464e71d7356374e98832cb6cd0",
     "grade": false,
     "grade_id": "cell-80134f1fef5eef35",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2857142857142857, 0.4444444444444444)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = 8/28 #replace with your answer\n",
    "precision = 8/18 #replace with your answer\n",
    "\n",
    "recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6b4d587ae3ea201f9ce4b2e05d2d2b31",
     "grade": true,
     "grade_id": "cell-27cb4abf121eb1fe",
     "locked": true,
     "points": 0.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(precision), float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4d5f6a701436594eb4c2f3cb14e9841a",
     "grade": true,
     "grade_id": "cell-399797a07036b377",
     "locked": true,
     "points": 0.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(recall), float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3c4ed0af0704a4a084dbf1f53113c05b",
     "grade": false,
     "grade_id": "cell-d2f88cf1c913bc0d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 8.2\n",
    "The balanced F measure (a.k.a. $F_1$) is defined as the harmonic mean of precision and recall. What is the advantage of using the harmonic mean rather than “averaging” (using the arithmetic mean)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d30fd647abab1986d0ef62ae8ec83c18",
     "grade": true,
     "grade_id": "cell-8c15f2522a286d6f",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ffac5ce81d6510d57a4b6654212936ba",
     "grade": false,
     "grade_id": "cell-eed045f5e6b3ee53",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 8.3\n",
    "Derive the equivalence between the two formulas for F measure shown in Equation (8.5), given that $\\alpha = 1/(\\beta^2 + 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "34d760e6602741ea5c8d9ec7f0dbf70f",
     "grade": true,
     "grade_id": "cell-53a0d302308961e4",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "870db5075de36f7db00aa1d0dd4a5391",
     "grade": false,
     "grade_id": "cell-2261bc4dd91291e7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 8.4\n",
    "What are the possible values for interpolated precision at a recall level of 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5ac0f0165254c66e09b35b5135de4cc4",
     "grade": true,
     "grade_id": "cell-cb243cc0dd24310e",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7111842d2595c012a3cd661b5d1b7003",
     "grade": false,
     "grade_id": "cell-d7783437d76e7668",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 8.8\n",
    "Consider an information need for which there are 4 relevant documents in the collection. Contrast two systems run on this collection. Their top 10 results are judged for relevance as follows (the leftmost item is the top ranked search result):\n",
    "\n",
    "| System   | R for relevant, N for nonrelevant |\n",
    "|----------|:--------------------|\n",
    "| System 1 | R N R N N N N N R R |\n",
    "| System 2 | N R N N R R R N N N |\n",
    "\n",
    "1. What is the MAP of each system? Which has a higher MAP?\n",
    "2. Does this result intuitively make sense? What does it say about what is important in getting a good MAP score?\n",
    "3. What is the R-precision of each system? (Does it rank the systems the same as MAP?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5b0a3edc87feea9822de168e236475db",
     "grade": false,
     "grade_id": "cell-c42b1a9a69260467",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "MAP_System1 = 1/1 # replace with your answer\n",
    "MAP_System2 = 1/1 # replace with your answer\n",
    "R_P_System1 = 1/1 # replace with your answer\n",
    "R_P_System2 = 1/1 # replace with your answer\n",
    "\n",
    "MAP_System1, MAP_System2, R_P_System1, R_P_System2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7068ae217f1658d8f06112e3e40fa774",
     "grade": true,
     "grade_id": "cell-2c0a902081d76545",
     "locked": true,
     "points": 0.25,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(MAP_System1), float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "633b3e964c4aa9a772bc873bf010aeb5",
     "grade": true,
     "grade_id": "cell-084f6e73ce2878f5",
     "locked": true,
     "points": 0.25,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(MAP_System2), float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5bc2ffe084a1acc42ab121385a18f7da",
     "grade": true,
     "grade_id": "cell-9614d1b2e6815021",
     "locked": true,
     "points": 0.25,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(R_P_System1), float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "858ea4d21f32f7db0eb4d7aec019f611",
     "grade": true,
     "grade_id": "cell-c8aefed6b2b8398e",
     "locked": true,
     "points": 0.25,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(R_P_System2), float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "773a7d83f5998b44de6f0af2d35ed94f",
     "grade": false,
     "grade_id": "88v",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 8.8 extra\n",
    "\n",
    "1. On the same data is used in the previous exercise, what would the MAP be when there would be 10 relevant documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "75df7a1552351d14bc2a622f20081d00",
     "grade": false,
     "grade_id": "88a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "MAP_System1ex = 1/1 # replace with your answer\n",
    "MAP_System2ex = 1/1 # replace with your answer\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "125540b05112f0e8ecea9458fa616f70",
     "grade": true,
     "grade_id": "88t",
     "locked": true,
     "points": 0.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(MAP_System1ex), float)\n",
    "assert_equal(type(MAP_System2ex), float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "145183e2b1ba270789d3a8f18cb14e85",
     "grade": false,
     "grade_id": "cell-1f877ea183d14013",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 8.9\n",
    "\n",
    "The following list of Rs and Ns represents relevant (R) and nonrelevant (N) returned documents in a ranked list of 20 documents retrieved in response to a query from a collection of 10,000 documents. The top of the ranked list (the document the system thinks is most likely to be relevant) is on the left of the list. This list shows 6 relevant documents. Assume that there are 8 relevant documents in total in the collection.\n",
    "\n",
    " > R R N N N $\\quad$ N N N R N $\\quad$ R N N N R $\\quad$ N N N N R\n",
    "\n",
    "1. What is the precision of the system on the top 20? (variable `precision`)\n",
    "2. What is the F1 on the top 20? (variable `F1`)\n",
    "3. What is the uninterpolated precision of the system at 25% recall? (variable `uninterpolated`)\n",
    "4. What is the interpolated precision at 33% recall? (variable `interpolated`)\n",
    "5. Assume that these 20 documents are the complete result set of the system. What is the AP for the query?<br/><br/>\n",
    "Assume, now, instead, that the system returned the entire 10,000 documents in a ranked list, and these are the first 20 results returned.<br/><br/>\n",
    "6. What is the largest possible AP that this system could have? (variable `largest_AP`)\n",
    "7. What is the smallest possible AP that this system could have? (variable `smallest_AP`)\n",
    "8. In a set of experiments, only the top 20 results are evaluated by hand. The result in (e) is used to approximate the range (f)–(g). For this example, how large (in absolute terms) can the error for the AP be by calculating (e) instead of (f) and (g) for this query? (variable `Error`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "15b06a1b04586c07b10c9cda022f8c03",
     "grade": false,
     "grade_id": "cell-6ececd8d73bfc0ca",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "precision = None\n",
    "F1 = None\n",
    "uninterpolated = None\n",
    "AP_query = None\n",
    "largest_AP = None\n",
    "smallest_AP = None\n",
    "Error = None\n",
    "\n",
    "precision, F1, uninterpolated, interpolated, AP_query, largest_AP, smallest_AP, Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "692448c0d136e60c5ced6e3abe8f4fd3",
     "grade": true,
     "grade_id": "cell-c2bb032e00c4dee1",
     "locked": true,
     "points": 0.2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(precision) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6d4b52d8e67f79a97cf5bf0a5f0b59ed",
     "grade": true,
     "grade_id": "cell-464b3522aa196af7",
     "locked": true,
     "points": 0.2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(F1) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "217d3651897af1c4b8fbee04ce310b18",
     "grade": true,
     "grade_id": "cell-ee2af88f21b18a7d",
     "locked": true,
     "points": 0.2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(uninterpolated) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e5d6fe375a8f2dec0a947b37f066af0b",
     "grade": true,
     "grade_id": "cell-dd18f398ad9a11ea",
     "locked": true,
     "points": 0.1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(AP_query) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8d4a9d3e1f376f843680fb044f6f79b5",
     "grade": true,
     "grade_id": "cell-43dee696647831ac",
     "locked": true,
     "points": 0.1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(largest_AP) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "eeae02e405950f380a48bdecbdc7a06d",
     "grade": true,
     "grade_id": "cell-6efcd2c3d9ed173c",
     "locked": true,
     "points": 0.1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(smallest_AP) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2d437ef9bd15ea427fc681b643afb534",
     "grade": true,
     "grade_id": "cell-dcd93ad480d010ac",
     "locked": true,
     "points": 0.1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(Error) in [float,int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3aef48cb18e14805cdb8213df5707eee",
     "grade": false,
     "grade_id": "cell-653e5c66190845a3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 8.10\n",
    "Below is a table showing how two human judges rated the relevance of a set of 12 documents to a particular information need (0 = nonrelevant, 1 = relevant). Let us assume that you’ve written an IR system that for this query returns the set of documents {4, 5, 6, 7, 8}\n",
    "\n",
    "| docId | Judge 1 | Judge 2|\n",
    "|------:|:--------|:-------|\n",
    "| 1     | 0       | 0      |\n",
    "| 2     | 0       | 0      |\n",
    "| 3     | 1       | 1      |\n",
    "| 4     | 1       | 1      |\n",
    "| 5     | 1       | 0      |\n",
    "| 6     | 1       | 0      |\n",
    "| 7     | 1       | 0      |\n",
    "| 8     | 1       | 0      |\n",
    "| 9     | 0       | 1      |\n",
    "| 10    | 0       | 1      |\n",
    "| 11    | 0       | 1      |\n",
    "| 12    | 0       | 1      |\n",
    "\n",
    "1. Calculate the kappa measure between the two judges.\n",
    "2. Calculate precision, recall, and F1 of your system if a document is considered relevant only if the two judges agree.\n",
    "3. Calculate precision, recall, and F1 of your system if a document is considered relevant if either judge thinks it is relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "eb4c1b27e6776637331d142902b8ccac",
     "grade": false,
     "grade_id": "cell-72efecf88879db26",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "kappa = None\n",
    "\n",
    "# if the two judges agree.\n",
    "precision1 =  None\n",
    "recall1 =  None\n",
    "F1score1 = None\n",
    "\n",
    "# if either judge thinks it is relevant.\n",
    "precision2 = None\n",
    "recall2 = None\n",
    "F1score2 = None\n",
    "\n",
    "\n",
    "print(kappa)\n",
    "print(precision1, recall1, F1score1)\n",
    "print(precision2, recall2, F1score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ad86d4c9429ec0ee3fe6b13676b9c72c",
     "grade": true,
     "grade_id": "cell-d0b49ee6b738d816",
     "locked": true,
     "points": 0.33,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(kappa) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "627faf4e76f87e5cb92feb67972bb9fa",
     "grade": true,
     "grade_id": "cell-6c4430b58ad99642",
     "locked": true,
     "points": 0.33,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(precision1) in [float,int]\n",
    "assert type(recall1) in [float,int]\n",
    "assert type(F1score1) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "df6af0266e84e0a9a1d839637e6bba72",
     "grade": true,
     "grade_id": "cell-b4ddeb8ad1122e7e",
     "locked": true,
     "points": 0.34,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(precision2) in [float,int]\n",
    "assert type(recall2) in [float,int]\n",
    "assert type(F1score2) in [float,int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "278dc547a0840b2625d16df9ef63fd62",
     "grade": false,
     "grade_id": "cell-db608e43be498940",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "28898816090b94a56863e26251744777",
     "grade": false,
     "grade_id": "em-v",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Evaluation measures\n",
    "\n",
    "1. Define a function `Rprecision(ranked_list_of_results,list_of_relevant_objects)` which does what it says, it returns the R-precision given the input data.\n",
    "2. Define a function `AveragePrecision(ranked_list_of_results,list_of_relevant_objects)` which returns the average precision of this list of results given the list of relevant answers.\n",
    "\n",
    "Both functions of course come with one or two well chosen tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "de18476927f4ff49054c47b24b947ed4",
     "grade": false,
     "grade_id": "em-a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Here you can add helper functions if needed or handy\n",
    "\n",
    "\n",
    "\n",
    "def Rprecision(ranked_list_of_results,list_of_relevant_objects):\n",
    "    \n",
    "    \n",
    "    \n",
    "def AveragePrecision(ranked_list_of_results, list_of_relevant_objects):\n",
    "    \n",
    "\n",
    "# Hint Define a couple of good tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9191f5774f4535d104d1fe42ae7b5db9",
     "grade": true,
     "grade_id": "em-t1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(Rprecision([1], [1]), float)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4d05f65deb46ae4ba034726c7805266c",
     "grade": true,
     "grade_id": "em-t2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(AveragePrecision([1], [1]), float)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bcac3e11641ce8c14ae19db88da51fc4",
     "grade": false,
     "grade_id": "pr",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Precision recall curve\n",
    "\n",
    "We create the interpolated precision-recall curve as in Figure 8.2 in MRS for one topic. We use for this the file \n",
    "`qrels.robust2004.txt`.\n",
    "\n",
    "It is easy to read in your data using pandas. We give some code to get you started.\n",
    "\n",
    "Store your answer in the dict `PR` using the provided schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2fc45da5ccfef9c0edd496c5df021e39",
     "grade": false,
     "grade_id": "prc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "qrels= pd.read_csv('qrels.robust2004.txt', sep=' ', header=None, \n",
    "                   names=['TOPIC',      'ITERATION',      'DOCUMENT_ID',     'RELEVANCY'])# https://trec.nist.gov/data/robust/qrels.robust2004.txt\n",
    "\n",
    "\n",
    "# Create Figure8\n",
    "\n",
    "rankedlist= qrels[(qrels.TOPIC==301)].DOCUMENT_ID.unique()\n",
    "np.random.shuffle(rankedlist) # This causes that the PR curve is different all the time (and often quite weird)\n",
    "relevantdocs=set(qrels[(qrels.TOPIC==301)&(qrels.RELEVANCY==1)].DOCUMENT_ID.unique() )\n",
    "\n",
    "Rlevels=[(r+1) /N for r in range(len(relevantdocs))] \n",
    "\n",
    "print('AP: topic 301', AveragePrecision(rankedlist,relevantdocs  ))\n",
    "\n",
    "qrels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "01027c61279685324d810cc33e4b7e55",
     "grade": false,
     "grade_id": "pra",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create Figure8.2 for topic 301\n",
    "       \n",
    " \n",
    "# You must calculate the correct precison values for every given recall value here\n",
    "PR={'UninterpolatedP':{r:.5 for r in Rlevels}, 'InterpolatedP':{r:.6 for r in Rlevels}}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.DataFrame(PR).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "326d67024d102de958f6dab1f96d765f",
     "grade": true,
     "grade_id": "prt",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert set(PR.keys())=={'UninterpolatedP', 'InterpolatedP'}\n",
    "for K in PR:\n",
    "     assert isinstance(PR[K],dict)\n",
    "assert_equal(set(PR['UninterpolatedP'].keys()),set(Rlevels))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d409e7643408910bcf2f768c2df01899",
     "grade": false,
     "grade_id": "ck-v",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Cohen's Kappa\n",
    "\n",
    "Compute the chance $P(E)$ used in the calculation of Cohen's Kappa, using the pooled marginals method.\n",
    "\n",
    "Your data looks like that given in Exercise 8.10: An array of pairs with binary relevance judgements in which the first column contains the scores of the first judge, etc.\n",
    "\n",
    "Hint: `numpy` arrays have all kind of handy methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a4416d9b5b37301b3a183748753f90b7",
     "grade": false,
     "grade_id": "ck-a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# input data size of kappa must be like this\n",
    "data= np.array([[1,1],[1,0],[0,1],[1,1],[0,0]])\n",
    "\n",
    "def PE(data):\n",
    "    '''On input `data`, return the   P(E) (expected agreement).'''\n",
    "     \n",
    "    data = np.array(data)\n",
    "    \n",
    "    return    P_E \n",
    "\n",
    "PE(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8cf21d47e9b7070cf58716b8627b85aa",
     "grade": true,
     "grade_id": "ck-t",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(PE(data), float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "627ad98ff8f230481ad539fd36a2df43",
     "grade": false,
     "grade_id": "cell-891a3bbce2d771fa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 10.1 Creating a test set\n",
    "\n",
    "Create a test collection in order to evaluate your two search engines (cosine similarity and BM25) on the set of Shakespeare. Create five information needs, which you express as queries. Use multi term queries only. For each information need you specify\n",
    "\n",
    " * The information need in natural language\n",
    " * The search query\n",
    " * A description when a document is considered relevant and when it is not. This should be a very clear description. A person assessing relevance may only use this together with the information need to determine relevance of a document. In particular the judge should not see the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "52040542d5f6c5c6fea8d35ad4a1ffb5",
     "grade": true,
     "grade_id": "cell-3fd2c07428004c67",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "16f16b8c95fcffd6029b5cfde151fa75",
     "grade": false,
     "grade_id": "cell-1af8400eea1e3e8e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 10.2 Assess search engine results\n",
    "\n",
    "Assess for your two search engines, for all five information needs the first 10 results given back by the systems. Let another student do the same task as well.\n",
    "1. Using your own relevance judgments, compute for each system, the precision at 10 for each query, and the average P@10. Discuss the results.\n",
    "2. You have now two times at least 50 relevance judgements, and probably more. Compute the kappa measure between the two judges as in exercise 8.10, using pooled marginals as in Table 8.2. Discuss your results and try to explain them.\n",
    "3. Now compute P@10 for both systems again, once using the relevance judgments of the other assessor, once in which you say that a document is relevant if BOTH judges said it was relevant, and once in which you say that a document is relevant if AT LEAST ONE of the judges said it was relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "668cb299c87264fda9c0bb50078702d1",
     "grade": true,
     "grade_id": "cell-1f5104381710e6d4",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "64b042a414ab4186ff36da8de10fc0ca",
     "grade": false,
     "grade_id": "cell-0828c47c6c841ab2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Programming (Inverted Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "45265903cced267bded49289c1a33748",
     "grade": false,
     "grade_id": "cell-ee70e59824b4530f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def loadShakespeare():\n",
    "    if 'shaks200.zip' in os.listdir():\n",
    "        return 'shaks200.zip'\n",
    "    elif os.path.exists('../../data/Week3/'):\n",
    "        return '../../data/Week3/shaks200.zip'\n",
    "    elif os.path.exists('../../../data/Week3/'):\n",
    "        return '../../../data/Week3/shaks200.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0a291330f776161fa4f004ba6e02d511",
     "grade": false,
     "grade_id": "cell-2cd22bbe0ed6e5b5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 12 Positional inverted index\n",
    "Adjust your software making an inverted index for the Shakespeare collection: the output should be a positional index. A positional index is an index that also stores the position of the occurrences of a word in a document. The following figure shows an example of a positional index.\n",
    "\n",
    "<img src=\"http://nlp.stanford.edu/IR-book/html/htmledition/img121.png\" />\n",
    "\n",
    "Make the index of the form (defaultdicts are also allowed!):\n",
    "```\n",
    "{'caesar':   {'files': \n",
    "                 {'lll': {'list': [25350], 'total': 1},\n",
    "                  'm_for_m': {'list': [6773, 14734], 'total': 2},\n",
    "                  'm_wives': {'list': [3459], 'total': 1},\n",
    "                  'macbeth': {'list': [9256], 'total': 1},\n",
    "                  'othello': {'list': [11728], 'total': 1},\n",
    "                  'rich_ii': {'list': [22695], 'total': 1},\n",
    "                  'rich_iii': {'list': [16418, 16570, 30799, 30801], 'total': 4},\n",
    "                  'titus': {'list': [368], 'total': 1},\n",
    "                  ....\n",
    "                  },\n",
    "               'total': 604}....\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5d6c8aae378a5c4105975e2845550b86",
     "grade": true,
     "grade_id": "cell-498f786fcde3299d",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def positional_inverted_index(zipfolder):\n",
    "    # With zipfile we can read the file without opening the zip file\n",
    "    archive = zipfile.ZipFile(zipfolder, 'r')\n",
    "    namelist = [x for x in archive.namelist() if '.xml' in x]\n",
    "    \n",
    "    for infile in tqdm_notebook(namelist): # loop over each file\n",
    "        \n",
    "                \n",
    "    return positional_index\n",
    "positional_index = positional_inverted_index(loadShakespeare())\n",
    "positional_index['ass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b6e21d8ffeb36779f0b33421a0797a57",
     "grade": true,
     "grade_id": "cell-29881806b7e5b184",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "positional_index = positional_inverted_index(loadShakespeare())\n",
    "assert type(positional_index) in [dict, defaultdict]\n",
    "assert_equal(set(positional_index['the'].keys()), {'total','files'})\n",
    "assert_equal(type(positional_index['the']['total']), int)\n",
    "assert_equal(set(positional_index['the']['files']['lll'].keys()), {'total','list'})\n",
    "assert_equal(type(positional_index['the']['files']['lll']['list']), list)\n",
    "assert_equal(type(positional_index['the']['files']['lll']['total']), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
