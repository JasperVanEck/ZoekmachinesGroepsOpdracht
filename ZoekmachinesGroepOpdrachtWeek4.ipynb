{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groep Opdracht Week 4 Zoekmachines\n",
    "\n",
    "## Students: Jasper van Eck, Ghislaine, Joris Galema, Lotte\n",
    "## Student IDs: 6228194, -, 11335165, 11269642\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "import nltk\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Open & read JSON file\n",
    "#Init empty list for json data to be stored\n",
    "jsonDataReviews = []\n",
    "with open('IMDB_reviews.json') as json_file:\n",
    "    #Loop through lines in json file, each review is on seperate line\n",
    "    for line in json_file:\n",
    "        #Append to the list of json data\n",
    "        jsonDataReviews.append(json.loads(line))\n",
    "\n",
    "#Read the data from the json file\n",
    "dataReviews = pd.DataFrame(jsonDataReviews)\n",
    "\n",
    "#Add Review_id column\n",
    "#Create index range\n",
    "review_id = list(range(len(dataReviews)))\n",
    "#Insert the index range into the DF\n",
    "dataReviews.insert(0,'review_id',review_id,True)\n",
    "#Cast to string from obj\n",
    "dataReviews['review_summary'] = dataReviews['review_summary'].astype(str)\n",
    "dataReviews['review_text'] = dataReviews['review_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>is_spoiler</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_summary</th>\n",
       "      <th>review_text</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>10</td>\n",
       "      <td>10 February 2006</td>\n",
       "      <td>A classic piece of unforgettable film-making.</td>\n",
       "      <td>In its Oscar year, Shawshank Redemption (writt...</td>\n",
       "      <td>ur1898687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>10</td>\n",
       "      <td>6 September 2000</td>\n",
       "      <td>Simply amazing. The best film of the 90's.</td>\n",
       "      <td>The Shawshank Redemption is without a doubt on...</td>\n",
       "      <td>ur0842118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>8</td>\n",
       "      <td>3 August 2001</td>\n",
       "      <td>The best story ever told on film</td>\n",
       "      <td>I believe that this film is the best story eve...</td>\n",
       "      <td>ur1285640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>10</td>\n",
       "      <td>1 September 2002</td>\n",
       "      <td>Busy dying or busy living?</td>\n",
       "      <td>**Yes, there are SPOILERS here**This film has ...</td>\n",
       "      <td>ur1003471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>8</td>\n",
       "      <td>20 May 2004</td>\n",
       "      <td>Great story, wondrously told and acted</td>\n",
       "      <td>At the heart of this extraordinary movie is a ...</td>\n",
       "      <td>ur0226855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>8</td>\n",
       "      <td>12 August 2004</td>\n",
       "      <td>Good , But It Is Overrated By Some</td>\n",
       "      <td>In recent years the IMDB top 250 movies has ha...</td>\n",
       "      <td>ur1532177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>9</td>\n",
       "      <td>9 October 2005</td>\n",
       "      <td>This Movie Saved My Life.</td>\n",
       "      <td>I have been a fan of this movie for a long tim...</td>\n",
       "      <td>ur6574726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>10</td>\n",
       "      <td>4 February 2012</td>\n",
       "      <td>Movie you can see 1000 times</td>\n",
       "      <td>I made my account on IMDb Just to Rate this mo...</td>\n",
       "      <td>ur31182745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>10</td>\n",
       "      <td>24 October 2008</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>A friend of mine listed \"The Shawshank Redempt...</td>\n",
       "      <td>ur9871443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>10</td>\n",
       "      <td>30 July 2011</td>\n",
       "      <td>\"I'm a convicted murderer who provides sound f...</td>\n",
       "      <td>Well I guess I'm a little late to the party as...</td>\n",
       "      <td>ur2707735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review_id  is_spoiler   movie_id rating       review_date  \\\n",
       "0          0        True  tt0111161     10  10 February 2006   \n",
       "1          1        True  tt0111161     10  6 September 2000   \n",
       "2          2        True  tt0111161      8     3 August 2001   \n",
       "3          3        True  tt0111161     10  1 September 2002   \n",
       "4          4        True  tt0111161      8       20 May 2004   \n",
       "5          5        True  tt0111161      8    12 August 2004   \n",
       "6          6        True  tt0111161      9    9 October 2005   \n",
       "7          7        True  tt0111161     10   4 February 2012   \n",
       "8          8        True  tt0111161     10   24 October 2008   \n",
       "9          9        True  tt0111161     10      30 July 2011   \n",
       "\n",
       "                                      review_summary  \\\n",
       "0      A classic piece of unforgettable film-making.   \n",
       "1         Simply amazing. The best film of the 90's.   \n",
       "2                   The best story ever told on film   \n",
       "3                         Busy dying or busy living?   \n",
       "4             Great story, wondrously told and acted   \n",
       "5                 Good , But It Is Overrated By Some   \n",
       "6                          This Movie Saved My Life.   \n",
       "7                       Movie you can see 1000 times   \n",
       "8                           The Shawshank Redemption   \n",
       "9  \"I'm a convicted murderer who provides sound f...   \n",
       "\n",
       "                                         review_text     user_id  \n",
       "0  In its Oscar year, Shawshank Redemption (writt...   ur1898687  \n",
       "1  The Shawshank Redemption is without a doubt on...   ur0842118  \n",
       "2  I believe that this film is the best story eve...   ur1285640  \n",
       "3  **Yes, there are SPOILERS here**This film has ...   ur1003471  \n",
       "4  At the heart of this extraordinary movie is a ...   ur0226855  \n",
       "5  In recent years the IMDB top 250 movies has ha...   ur1532177  \n",
       "6  I have been a fan of this movie for a long tim...   ur6574726  \n",
       "7  I made my account on IMDb Just to Rate this mo...  ur31182745  \n",
       "8  A friend of mine listed \"The Shawshank Redempt...   ur9871443  \n",
       "9  Well I guess I'm a little late to the party as...   ur2707735  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of data\n",
    "dataReviews.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the TF Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Init a default dict\n",
    "tfDict = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "#Init Porter Stemmer\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "#Use less reviews to reduce runtimes for testing/practice\n",
    "dataReviewsLess = pd.DataFrame(dataReviews.head(10000))\n",
    "\n",
    "#Retrieve the actual reviews\n",
    "reviewTexts = dataReviewsLess['review_text'].values\n",
    "\n",
    "#Loop through reviews\n",
    "for i in range(len(reviewTexts)):\n",
    "    #Tokenize reviews and lowercase the text\n",
    "    line = re.split('\\W+',reviewTexts[i].lower())\n",
    "    #Loop through tokens in review\n",
    "    for word in line:\n",
    "        #Stem token\n",
    "        stem = ps.stem(word)\n",
    "        #Increment frequency\n",
    "        tfDict[stem][i] += 1\n",
    "\n",
    "#Add in Corpus Frequency, Document Frequency and reposition the frequencies per document\n",
    "tfDictXtra = defaultdict(lambda: defaultdict(int))\n",
    "for word in tfDict:\n",
    "    tfDictXtra[word]['CorpusFreq'] = sum(tfDict[word].values())\n",
    "    tfDictXtra[word]['DocFreq'] = len(tfDict[word])\n",
    "    tfDictXtra[word]['Freq_per_doc'] = tfDict[word]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the TF-IDF and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Init a default dict\n",
    "tfDict = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "#Init Porter Stemmer\n",
    "ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "#Use less reviews to reduce runtimes for testing/practice\n",
    "dataReviewsLess = pd.DataFrame(dataReviews.head(10000))\n",
    "\n",
    "#Retrieve the actual reviews\n",
    "reviewTexts = dataReviewsLess['review_text'].values\n",
    "\n",
    "#Loop through reviews\n",
    "for i in range(len(reviewTexts)):\n",
    "    #Tokenize reviews and lowercase the text\n",
    "    line = re.split('\\W+',reviewTexts[i].lower())\n",
    "    #Loop through tokens in review\n",
    "    for word in line:\n",
    "        #Stem token\n",
    "        stem = ps.stem(word)\n",
    "        #Increment frequency\n",
    "        tfDict[stem][i] += 1\n",
    "\n",
    "#Add in Corpus Frequency, Document Frequency and reposition the frequencies per document\n",
    "tfDictXtra = defaultdict(lambda: defaultdict(int))\n",
    "for word in tfDict:\n",
    "    tfDictXtra[word]['CorpusFreq'] = sum(tfDict[word].values())\n",
    "    tfDictXtra[word]['DocFreq'] = len(tfDict[word])\n",
    "    tfDictXtra[word]['Freq_per_doc'] = tfDict[word]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the TF-IDF and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the total number of reviews/documents\n",
    "totalDocs = len(dataReviewsLess)\n",
    "\n",
    "#Total unique words\n",
    "totalUniqueWords = len(tfDictXtra)\n",
    "\n",
    "#Create np matrix with zeros\n",
    "tfIdf = np.zeros((totalUniqueWords,totalDocs))\n",
    "\n",
    "#Create dataframe of words with index list to get the word position in matrix for future reference\n",
    "wordsIndex = pd.DataFrame(list(tfDictXtra.keys()),columns=['Words'])\n",
    "#Create index range\n",
    "wordID = list(range(totalUniqueWords))\n",
    "#Insert the index range\n",
    "wordsIndex.insert(0,'Index',wordID,True)\n",
    "#Index counter, to keep track of location in word list\n",
    "wordCounter = 0\n",
    "\n",
    "\n",
    "#loop through words in dict\n",
    "for word in tfDictXtra:\n",
    "    #Loop through frequencies of word in a doc from dict; LET OP deze regel geeft soms AttributeError: 'int' object has no attribute 'keys'\n",
    "    #run de vorige cellen dan weer even opnieuw. Dat verhelpt t meestal\n",
    "    dictLoop = list(tfDictXtra[word]['Freq_per_doc'].keys())\n",
    "    for doc in dictLoop:\n",
    "        #Calculate the TF-IDF\n",
    "        tfIdf[wordCounter,doc] = tfDictXtra[word]['Freq_per_doc'][doc]*math.log((totalDocs/(1+tfDictXtra[word]['DocFreq'])))\n",
    "    wordCounter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transpose the tfIdf matrix and normalize, since the normalize works on rows, and we need to normalize the columns\n",
    "tfIdfNorm = preprocessing.normalize(tfIdf.T, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting/test query\n",
    "query = \"Shawshank redemption interesting cool\"\n",
    "\n",
    "#Create a normalized vector of query\n",
    "def vectorizeQuery(query):\n",
    "    #Create empty base vector for Term Freq\n",
    "    queryVector = np.zeros(totalUniqueWords)\n",
    "    #Tokenize and make lowercase\n",
    "    line = re.split('\\W+',query.lower())\n",
    "    #Loop through words\n",
    "    for word in line:\n",
    "        #Stem each word\n",
    "        stem = ps.stem(word)\n",
    "        #Increase term freq of query term\n",
    "        queryVector[wordsIndex[wordsIndex['Words']==stem]['Index'].values] += 1\n",
    "    \n",
    "    #Create empty base vector for TF-IDF\n",
    "    queryVectorTfIdf = np.zeros(totalUniqueWords)\n",
    "    #Loop through TF vector of query\n",
    "    for i in range(len(queryVector)):\n",
    "        #Act where a term frequency was recorded\n",
    "        if queryVector[i] != 0:\n",
    "            #Determine the which word it was based on the index\n",
    "            word = str(wordsIndex[wordsIndex['Index']==i]['Words'].values)\n",
    "            #Calculate the TF-IDF\n",
    "            queryVectorTfIdf[i] = queryVector[i]*math.log((totalDocs/(1+tfDictXtra[word]['DocFreq'])))\n",
    "    \n",
    "    #Make the TF-IDF vector a unit vector\n",
    "    length = np.sqrt(queryVectorTfIdf.dot(queryVectorTfIdf))\n",
    "    queryVectorNorm = queryVectorTfIdf/length\n",
    "    \n",
    "    #Return the unit vector\n",
    "    return queryVectorNorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cosine similarity matching\n",
    "def cosineSim(vector, docVector):\n",
    "    #Only dot product needed since vectors are already unit vectors and therefore the lengths are 1\n",
    "    return vector.dot(docVector)#/(length vector * length docVector)\n",
    "    \n",
    "def rankedList(queryVector):\n",
    "    #Create empty score list\n",
    "    scoreList = np.zeros(totalDocs)\n",
    "    #Loop through each doc\n",
    "    for i in range(len(tfIdfNorm)):\n",
    "        #Calculate for each doc the cosine sim. Index of scoreList = review_id\n",
    "        scoreList[i] = cosineSim(queryVector,tfIdfNorm[i])\n",
    "    \n",
    "    #Create new data frame for ranked list based on smaller DF of data\n",
    "    rankedDocList = pd.DataFrame(dataReviewsLess)\n",
    "    #Insert the similarity score for each review\n",
    "    rankedDocList.insert(0,'Score',scoreList,True)\n",
    "    #Sort the review similarity based on the score and return\n",
    "    return rankedDocList.sort_values(by='Score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the ranking list\n",
    "rankings = rankedList(vectorizeQuery(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b39669ed6f34f53bb23c815b3135585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='x', min=10, step=10), Output()), _dom_classes=('widget-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def showResults(x=(10, 100, 10)):\n",
    "    #Show first x results of the rankings\n",
    "    return rankings.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6abe7c86b08b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# input data size of kappa must be like this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def AveragePrecision(ranked_list_of_results, list_of_relevant_objects):\n",
    "    begin = 1/len(list_of_relevant_objects)\n",
    "    count = 0\n",
    "    for i, res in enumerate(ranked_list_of_results):\n",
    "        for j, obj in enumerate(list_of_relevant_objects):\n",
    "            if obj == res:\n",
    "                itera = (j+1) / (i+1)\n",
    "            count = count + itera\n",
    "    return begin * count\n",
    "\n",
    "def PE(data):\n",
    "    '''On input data, return the P(E) (expected agreement).'''\n",
    "    relevant = 0\n",
    "    nonrelevant = 0\n",
    "    # Iterate over the data\n",
    "    for i in data:\n",
    "        for j in i:\n",
    "            \n",
    "            # Top up the relevant documents by one if 1 is encountered\n",
    "            if j == 1:\n",
    "                relevant += 1\n",
    "            # Top up the nonrelevant documents by one if 0 is encountered\n",
    "            if j == 0:\n",
    "                nonrelevant += 1\n",
    "\n",
    "    # Calculates the total of inspected documents for the judges combined\n",
    "    total = len(data)*2\n",
    "\n",
    "    # Calculates the pooled marginals\n",
    "    rel = relevant/total\n",
    "    nonrel = nonrelevant/total\n",
    "\n",
    "    # Calculates the P(E)\n",
    "    P_E = nonrel**2 + rel **2    \n",
    "    return    P_E \n",
    "\n",
    "\n",
    "def kappa(data, P_E):\n",
    "    agree = 0\n",
    "    for i in data:\n",
    "        temp = None\n",
    "        for j in i:\n",
    "            if temp == j:\n",
    "                agree += 1\n",
    "            temp = j\n",
    "    P_A = agree / len(data)\n",
    "    kappa = (P_A - P_E)/(1 - P_E)   \n",
    "    return kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine similarity matching\n",
    "def cosineSim(vector, docVector):\n",
    "    #Only dot product needed since vectors are already unit vectors and therefore the lengths are 1\n",
    "    return vector.dot(docVector)#/(length vector * length docVector)\n",
    "    \n",
    "def rankedList(queryVector):\n",
    "    #Create empty score list\n",
    "    scoreList = np.zeros(totalDocs)\n",
    "    #Loop through each doc\n",
    "    for i in range(len(tfIdfNorm)):\n",
    "        #Calculate for each doc the cosine sim. Index of scoreList = review_id\n",
    "        scoreList[i] = cosineSim(queryVector,tfIdfNorm[i])\n",
    "    \n",
    "    #Create new data frame for ranked list based on smaller DF of data\n",
    "    rankedDocList = pd.DataFrame(dataReviewsLess)\n",
    "    #Insert the similarity score for each review\n",
    "    rankedDocList.insert(0,'Score',scoreList,True)\n",
    "    #Sort the review similarity based on the score and return\n",
    "    return rankedDocList.sort_values(by='Score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the ranking list\n",
    "rankings = rankedList(vectorizeQuery(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b39669ed6f34f53bb23c815b3135585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='x', min=10, step=10), Output()), _dom_classes=('widget-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def showResults(x=(10, 100, 10)):\n",
    "    #Show first x results of the rankings\n",
    "    return rankings.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
